{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a02dd87",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import all necessary libraries for deep learning, data processing, and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6eae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['ABSL_LOG_LEVEL'] = 'FATAL'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ac96a",
   "metadata": {},
   "source": [
    "## 2. Set Random Seeds\n",
    "Ensure reproducibility of experiments by setting consistent random seeds for all operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceca1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b74a0",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Augmentation\n",
    "Define image data generators with normalization, rotation, flipping, and other data augmentation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15eaa853",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def pytorch_normalize(img):\n",
    "    img = img / 255.0\n",
    "    return (img - mean) / std\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=pytorch_normalize,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=[0.95, 1.05],\n",
    "    brightness_range=[0.85, 1.15],\n",
    "    horizontal_flip=True,\n",
    "    channel_shift_range=0.02,\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=pytorch_normalize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ac557",
   "metadata": {},
   "source": [
    "## 4. Create Data Generators\n",
    "Create data generators for training, validation, and test sets, and display dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986483b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15806 images belonging to 10 classes.\n",
      "Found 2963 images belonging to 10 classes.\n",
      "Found 993 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../data/garbage-dataset'\n",
    "train_dir = '../data/garbage-split/train'\n",
    "val_dir = '../data/garbage-split/val'\n",
    "test_dir = '../data/garbage-split/test'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=True, seed=SEED\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    val_dir, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    test_dir, target_size=(224, 224), batch_size=32, class_mode='categorical',shuffle=False\n",
    ")\n",
    "\n",
    "train_steps = int(np.ceil(train_generator.samples / 32))\n",
    "val_steps = int(np.ceil(val_generator.samples / 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96c9ed",
   "metadata": {},
   "source": [
    "## 5. Build MobileNetV2 Model\n",
    "Use pre-trained MobileNetV2 as base model and add custom classification layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b09f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6471b1",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "Train the model using callbacks including early stopping, learning rate reduction, and model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5af47e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric-u/miniconda3/envs/tf216/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - accuracy: 0.6587 - loss: 1.0668\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88019, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 378ms/step - accuracy: 0.6590 - loss: 1.0660 - val_accuracy: 0.8802 - val_loss: 0.3676 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.8858 - loss: 0.3535\n",
      "Epoch 2: val_accuracy improved from 0.88019 to 0.92204, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 338ms/step - accuracy: 0.8858 - loss: 0.3534 - val_accuracy: 0.9220 - val_loss: 0.2598 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - accuracy: 0.9224 - loss: 0.2384\n",
      "Epoch 3: val_accuracy improved from 0.92204 to 0.92744, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 367ms/step - accuracy: 0.9224 - loss: 0.2384 - val_accuracy: 0.9274 - val_loss: 0.2430 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.9427 - loss: 0.1768\n",
      "Epoch 4: val_accuracy did not improve from 0.92744\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 356ms/step - accuracy: 0.9427 - loss: 0.1768 - val_accuracy: 0.9244 - val_loss: 0.2492 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - accuracy: 0.9565 - loss: 0.1383\n",
      "Epoch 5: val_accuracy improved from 0.92744 to 0.93318, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 346ms/step - accuracy: 0.9565 - loss: 0.1383 - val_accuracy: 0.9332 - val_loss: 0.2265 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.9632 - loss: 0.1130\n",
      "Epoch 6: val_accuracy improved from 0.93318 to 0.93925, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 347ms/step - accuracy: 0.9632 - loss: 0.1130 - val_accuracy: 0.9393 - val_loss: 0.2144 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.9737 - loss: 0.0849\n",
      "Epoch 7: val_accuracy did not improve from 0.93925\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 348ms/step - accuracy: 0.9737 - loss: 0.0849 - val_accuracy: 0.9291 - val_loss: 0.2651 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.9716 - loss: 0.0860\n",
      "Epoch 8: val_accuracy improved from 0.93925 to 0.93993, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 338ms/step - accuracy: 0.9716 - loss: 0.0860 - val_accuracy: 0.9399 - val_loss: 0.2307 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.9782 - loss: 0.0705\n",
      "Epoch 9: val_accuracy did not improve from 0.93993\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 342ms/step - accuracy: 0.9782 - loss: 0.0706 - val_accuracy: 0.9396 - val_loss: 0.2291 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.9774 - loss: 0.0681\n",
      "Epoch 10: val_accuracy did not improve from 0.93993\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 347ms/step - accuracy: 0.9774 - loss: 0.0681 - val_accuracy: 0.9372 - val_loss: 0.2487 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - accuracy: 0.9841 - loss: 0.0478\n",
      "Epoch 11: val_accuracy improved from 0.93993 to 0.94128, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 337ms/step - accuracy: 0.9841 - loss: 0.0478 - val_accuracy: 0.9413 - val_loss: 0.2449 - learning_rate: 5.0000e-05\n",
      "Epoch 12/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.9882 - loss: 0.0356\n",
      "Epoch 12: val_accuracy improved from 0.94128 to 0.94499, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 363ms/step - accuracy: 0.9882 - loss: 0.0356 - val_accuracy: 0.9450 - val_loss: 0.2378 - learning_rate: 5.0000e-05\n",
      "Epoch 13/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.9906 - loss: 0.0275\n",
      "Epoch 13: val_accuracy did not improve from 0.94499\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 382ms/step - accuracy: 0.9906 - loss: 0.0275 - val_accuracy: 0.9420 - val_loss: 0.2397 - learning_rate: 5.0000e-05\n",
      "Epoch 14/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - accuracy: 0.9907 - loss: 0.0280\n",
      "Epoch 14: val_accuracy did not improve from 0.94499\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 343ms/step - accuracy: 0.9907 - loss: 0.0280 - val_accuracy: 0.9426 - val_loss: 0.2463 - learning_rate: 5.0000e-05\n",
      "Epoch 15/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.9931 - loss: 0.0219\n",
      "Epoch 15: val_accuracy improved from 0.94499 to 0.94803, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 349ms/step - accuracy: 0.9931 - loss: 0.0219 - val_accuracy: 0.9480 - val_loss: 0.2204 - learning_rate: 2.5000e-05\n",
      "Epoch 16/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 0.9926 - loss: 0.0254\n",
      "Epoch 16: val_accuracy did not improve from 0.94803\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 339ms/step - accuracy: 0.9926 - loss: 0.0254 - val_accuracy: 0.9470 - val_loss: 0.2224 - learning_rate: 2.5000e-05\n",
      "Epoch 17/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.9928 - loss: 0.0211\n",
      "Epoch 17: val_accuracy did not improve from 0.94803\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 345ms/step - accuracy: 0.9928 - loss: 0.0211 - val_accuracy: 0.9470 - val_loss: 0.2258 - learning_rate: 2.5000e-05\n",
      "Epoch 18/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.9943 - loss: 0.0192\n",
      "Epoch 18: val_accuracy did not improve from 0.94803\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 342ms/step - accuracy: 0.9943 - loss: 0.0192 - val_accuracy: 0.9474 - val_loss: 0.2293 - learning_rate: 2.5000e-05\n",
      "Epoch 19/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - accuracy: 0.9942 - loss: 0.0154\n",
      "Epoch 19: val_accuracy did not improve from 0.94803\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 391ms/step - accuracy: 0.9942 - loss: 0.0154 - val_accuracy: 0.9474 - val_loss: 0.2257 - learning_rate: 1.2500e-05\n",
      "Epoch 20/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - accuracy: 0.9961 - loss: 0.0137\n",
      "Epoch 20: val_accuracy did not improve from 0.94803\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 390ms/step - accuracy: 0.9961 - loss: 0.0137 - val_accuracy: 0.9480 - val_loss: 0.2251 - learning_rate: 1.2500e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.9970 - loss: 0.0104\n",
      "Epoch 21: val_accuracy improved from 0.94803 to 0.94938, saving model to saved_models/best_mobilenetv2.keras\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 352ms/step - accuracy: 0.9970 - loss: 0.0104 - val_accuracy: 0.9494 - val_loss: 0.2277 - learning_rate: 1.2500e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.9971 - loss: 0.0102\n",
      "Epoch 22: val_accuracy did not improve from 0.94938\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 360ms/step - accuracy: 0.9971 - loss: 0.0102 - val_accuracy: 0.9480 - val_loss: 0.2273 - learning_rate: 1.2500e-05\n",
      "Epoch 23/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 0.9949 - loss: 0.0125\n",
      "Epoch 23: val_accuracy did not improve from 0.94938\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 346ms/step - accuracy: 0.9949 - loss: 0.0125 - val_accuracy: 0.9477 - val_loss: 0.2312 - learning_rate: 6.2500e-06\n",
      "Epoch 24/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 0.9961 - loss: 0.0117\n",
      "Epoch 24: val_accuracy did not improve from 0.94938\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 339ms/step - accuracy: 0.9961 - loss: 0.0117 - val_accuracy: 0.9490 - val_loss: 0.2307 - learning_rate: 6.2500e-06\n",
      "Epoch 25/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - accuracy: 0.9976 - loss: 0.0077\n",
      "Epoch 25: val_accuracy did not improve from 0.94938\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 348ms/step - accuracy: 0.9976 - loss: 0.0077 - val_accuracy: 0.9463 - val_loss: 0.2358 - learning_rate: 6.2500e-06\n",
      "Epoch 26/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 0.9963 - loss: 0.0104\n",
      "Epoch 26: val_accuracy did not improve from 0.94938\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 349ms/step - accuracy: 0.9963 - loss: 0.0104 - val_accuracy: 0.9477 - val_loss: 0.2332 - learning_rate: 6.2500e-06\n",
      "Epoch 27/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.9976 - loss: 0.0092\n",
      "Epoch 27: val_accuracy did not improve from 0.94938\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 362ms/step - accuracy: 0.9976 - loss: 0.0092 - val_accuracy: 0.9470 - val_loss: 0.2336 - learning_rate: 3.1250e-06\n",
      "Epoch 28/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - accuracy: 0.9969 - loss: 0.0108\n",
      "Epoch 28: val_accuracy did not improve from 0.94938\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 362ms/step - accuracy: 0.9969 - loss: 0.0108 - val_accuracy: 0.9480 - val_loss: 0.2340 - learning_rate: 3.1250e-06\n",
      "Epoch 29/30\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.9960 - loss: 0.0133\n",
      "Epoch 29: val_accuracy did not improve from 0.94938\n",
      "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 358ms/step - accuracy: 0.9960 - loss: 0.0133 - val_accuracy: 0.9490 - val_loss: 0.2355 - learning_rate: 3.1250e-06\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"saved_models/best_mobilenetv2.keras\",  monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True, min_delta=0.001, verbose=1)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps,\n",
    "    epochs=30,\n",
    "    callbacks=[checkpoint, early_stop, lr_reduce],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(\"saved_models/mobilenetv2_final.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c2b37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 176ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       1.00      1.00      1.00        48\n",
      "  biological       1.00      0.92      0.96        50\n",
      "   cardboard       0.99      0.92      0.96        92\n",
      "     clothes       0.99      0.99      0.99       267\n",
      "       glass       0.99      0.95      0.97       154\n",
      "       metal       0.94      0.96      0.95        51\n",
      "       paper       0.87      0.98      0.92        84\n",
      "     plastic       0.93      0.98      0.96       100\n",
      "       shoes       0.93      0.97      0.95        99\n",
      "       trash       0.93      0.90      0.91        48\n",
      "\n",
      "    accuracy                           0.96       993\n",
      "   macro avg       0.96      0.96      0.96       993\n",
      "weighted avg       0.96      0.96      0.96       993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=list(test_generator.class_indices.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf216",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
